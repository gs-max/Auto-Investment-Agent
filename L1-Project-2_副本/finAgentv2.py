import asyncio
import functools
from pprint import pprint
from langchain_core.messages import HumanMessage, AIMessage
# ä» langgraph.checkpoint.aiopg å¯¼å…¥
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver
from langgraph.store.postgres.aio import AsyncPostgresStore
from langchain_mcp_adapters.client import MultiServerMCPClient
# å¯¼å…¥æ—¥å¿—æ¨¡å—ï¼Œç”¨äºè®°å½•ç¨‹åºè¿è¡Œæ—¶çš„ä¿¡æ¯
import logging
from concurrent_log_handler import ConcurrentRotatingFileHandler
# å¯¼å…¥æ“ä½œç³»ç»Ÿæ¥å£æ¨¡å—ï¼Œç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„å’Œç¯å¢ƒå˜é‡
import os
import sys
import threading
import time
# å¯¼å…¥UUIDæ¨¡å—ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
import uuid
# ä»htmlæ¨¡å—å¯¼å…¥escapeå‡½æ•°ï¼Œç”¨äºè½¬ä¹‰HTMLç‰¹æ®Šå­—ç¬¦
from html import escape
# ä»typingæ¨¡å—å¯¼å…¥ç±»å‹æç¤ºå·¥å…·
from typing import Literal, Annotated, Sequence, Optional, Any
# ä»typing_extensionså¯¼å…¥TypedDictï¼Œç”¨äºå®šä¹‰ç±»å‹åŒ–çš„å­—å…¸
from typing_extensions import TypedDict
# å¯¼å…¥LangChainçš„æç¤ºæ¨¡æ¿ç±»
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
# å¯¼å…¥LangChainçš„æ¶ˆæ¯åŸºç±»
from langchain_core.messages import BaseMessage
# å¯¼å…¥æ¶ˆæ¯å¤„ç†å‡½æ•°ï¼Œç”¨äºè¿½åŠ æ¶ˆæ¯
from langgraph.graph.message import add_messages
# å¯¼å…¥é¢„æ„å»ºçš„å·¥å…·æ¡ä»¶å’Œå·¥å…·èŠ‚ç‚¹
from langgraph.prebuilt import tools_condition, ToolNode
from concurrent.futures import ThreadPoolExecutor, as_completed
from langchain_core.messages import ToolMessage
# å¯¼å…¥çŠ¶æ€å›¾å’Œèµ·å§‹/ç»“æŸèŠ‚ç‚¹çš„å®šä¹‰
from langgraph.graph import StateGraph, START, END
# å¯¼å…¥åŸºç¡€å­˜å‚¨æ¥å£
from langgraph.store.base import BaseStore
# å¯¼å…¥å¯è¿è¡Œé…ç½®ç±»
from langchain_core.runnables import RunnableConfig
# å¯¼å…¥Postgreså­˜å‚¨ç±»
from langgraph.store.postgres import PostgresStore
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
# å¯¼å…¥ psycopg2 çš„æ“ä½œå¼‚å¸¸ç±»ï¼Œç”¨äºæ•è·æ•°æ®åº“è¿æ¥é”™è¯¯
from psycopg2 import OperationalError
# å¯¼å…¥Postgresæ£€æŸ¥ç‚¹ä¿å­˜ç±»
from langgraph.checkpoint.postgres import PostgresSaver
# å¯¼å…¥PostgreSQLè¿æ¥æ± ç±»
from psycopg_pool import ConnectionPool, AsyncConnectionPool
# å¯¼å…¥Pydanticçš„åŸºç±»å’Œå­—æ®µå®šä¹‰å·¥å…·
from pydantic import BaseModel, Field, model_validator
# å¯¼å…¥è‡ªå®šä¹‰çš„get_llmå‡½æ•°ï¼Œç”¨äºè·å–LLMæ¨¡å‹
from utils.llms import get_llm
# å¯¼å…¥ç»Ÿä¸€çš„ Config ç±»
from utils.config import Config
from typing import List
# # è®¾ç½®æ—¥å¿—åŸºæœ¬é…ç½®ï¼Œçº§åˆ«ä¸ºDEBUGæˆ–INFO
logger = logging.getLogger(__name__)
# è®¾ç½®æ—¥å¿—å™¨çº§åˆ«ä¸ºDEBUG
logger.setLevel(logging.DEBUG)
# logger.setLevel(logging.INFO)
logger.handlers = []  # æ¸…ç©ºé»˜è®¤å¤„ç†å™¨
# ä½¿ç”¨ConcurrentRotatingFileHandler
# å®šä¹‰æ¶ˆæ¯çŠ¶æ€ç±»ï¼Œä½¿ç”¨TypedDictè¿›è¡Œç±»å‹æ³¨è§£

PLANNER_EXAMPLES = [
    # ç¤ºä¾‹1: å¤æ‚æŸ¥è¯¢
    ("human", "æ ¹æ®ç ”æŠ¥ï¼Œå¯¹æ¯”ä¸€ä¸‹å®å¾·æ—¶ä»£å’Œç‰¹æ–¯æ‹‰çš„é£é™©ï¼Œå¹¶åˆ†åˆ«å‘Šè¯‰æˆ‘å®ƒä»¬ä¿©çš„æœ€æ–°è‚¡ä»·ã€‚"),
    ("ai", """
    {
        "plan": {
            "thought": "The user wants a risk comparison from the report and real-time stock prices for two different companies from different markets. I need four steps: search the report for CATL's risks, search for Tesla's risks, get CATL's A-share price, and get Tesla's US stock price. I must use the correct price tool for each market.",
            "tasks": [
                {"task_id": 1, "tool_name": "search_financial_reports", "tool_args": {"query": "å®å¾·æ—¶ä»£çš„é£é™©"}, "question": "ç ”æŠ¥ä¸­æåˆ°äº†å®å¾·æ—¶ä»£çš„å“ªäº›é£é™©ï¼Ÿ"},
                {"task_id": 2, "tool_name": "search_financial_reports", "tool_args": {"query": "ç‰¹æ–¯æ‹‰çš„é£é™©"}, "question": "ç ”æŠ¥ä¸­æåˆ°äº†ç‰¹æ–¯æ‹‰çš„å“ªäº›é£é™©ï¼Ÿ"},
                {"task_id": 3, "tool_name": "get_internal_stock_price", "tool_args": {"symbol": "å®å¾·æ—¶ä»£"}, "question": "å®å¾·æ—¶ä»£çš„æœ€æ–°Aè‚¡è‚¡ä»·æ˜¯å¤šå°‘ï¼Ÿ"},
                {"task_id": 4, "tool_name": "get_international_financial_product_price", "tool_args": {"symbol": "TSLA"}, "question": "ç‰¹æ–¯æ‹‰(TSLA)çš„æœ€æ–°è‚¡ä»·æ˜¯å¤šå°‘ï¼Ÿ"}
            ]
        },
        "chat_response": null
    }
    """),
    # ç¤ºä¾‹2: ç®€å•RAGæŸ¥è¯¢
    ("human", "è¿™ä»½æŠ¥å‘Šçš„æ ¸å¿ƒè§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"),
    ("ai", """
    {
        "plan": {
            "thought": "The user is asking for the main summary of the report. A single step is sufficient.",
            "tasks": [
                {"task_id": 1, "tool_name": "search_financial_reports", "tool_args": {"query": "æŠ¥å‘Šçš„æ ¸å¿ƒè§‚ç‚¹å’Œæ‘˜è¦"}, "question": "è¿™ä»½æŠ¥å‘Šçš„æ ¸å¿ƒè§‚ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"}
            ]
        },
        "chat_response": null
    }
    """),
    # ç¤ºä¾‹3: ç®€å•èŠå¤©
    ("human", "ä½ å¥½"),
    ("ai", """
    {
        "plan": null,
        "chat_response": "æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„ç ”æŠ¥åˆ†æåŠ©æ‰‹ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"
    }
    """)
]


class Reflection(BaseModel):
    """
    æ‰¿è½½å¯¹å·¥å…·æ‰§è¡Œç»“æœçš„æ·±åº¦åæ€ã€‚
    """
    assessment: Literal["success", "failure", "partial_success"] = Field(
        description="Assessment of the tool execution. 'success' if the answer is complete and accurate. 'partial_success' if the information is useful but incomplete. 'failure' if the result is an error, irrelevant, or contains no information."
    )
    
    reasoning: str = Field(
        description="A brief, critical explanation for the assessment. Explain what was good or what was missing."
    )
    
    suggestion_for_next_step: Optional[str] = Field(
        default=None,
        description="Actionable suggestion for the next step if assessment is not 'success'. E.g., 'Retry with a broader search term like financial risks', or 'The stock symbol might be wrong, try searching for the company name'."
    )
    
    # æ–°å¢å­—æ®µ
    is_sufficient: bool = Field(
        description="A boolean flag indicating if the information obtained is sufficient to stop the process, even if not a complete success."
    )

class SubTask(BaseModel):
    task_id: int = Field(description="Unique ID for the sub-task, starting from 1.")
    tool_name: str = Field(description="The name of the tool to be called for this task.")
    tool_args: dict = Field(description="The arguments to be passed to the tool.")
    question: str = Field(description="The natural language question this sub-task aims to answer.")
    status: str = Field(default="pending", description="Status of the task: pending, completed, failed.")
    result: Optional[str] = Field(default=None, description="The result obtained after executing the tool.")

class Plan(BaseModel):
    thought: str = Field(description="A brief summary of the overall plan and reasoning.")
    tasks: List[SubTask] = Field(description="A list of sub-tasks to be executed.")



class PlannerOutput(BaseModel):
    """
    ä¸€ä¸ªå¯ä»¥åŒ…å«è®¡åˆ’æˆ–ç›´æ¥èŠå¤©å›å¤çš„è¾“å‡ºæ¨¡å‹ã€‚
    è¿™ä¸¤è€…å¿…é¡»æ˜¯äº’æ–¥çš„ã€‚
    """
    
    plan: Optional[Plan] = Field(
        default=None, 
        description="A detailed, step-by-step plan if the query requires tool use. Should be null if a direct chat response is provided."
    )
    
    chat_response: Optional[str] = Field(
        default=None, 
        description="A direct response to the user if the query is a simple chat message that does not require any tools. Should be null if a plan is generated."
    )

    # ä½¿ç”¨ @model_validator å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡ŒéªŒè¯
    @model_validator(mode='after')
    def check_plan_or_response_exclusive(self) -> 'PlannerOutput':
        # åœ¨ 'after' æ¨¡å¼ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥è®¿é—® selfï¼Œå³æ¨¡å‹å®ä¾‹
        plan_exists = self.plan is not None
        response_exists = self.chat_response is not None

        # è§„åˆ™1: ä¸èƒ½ä¸¤ä¸ªéƒ½å­˜åœ¨
        if plan_exists and response_exists:
            raise ValueError("Either 'plan' or 'chat_response' can be provided, but not both.")
        
        # è§„åˆ™2: å¿…é¡»æœ‰ä¸€ä¸ªå­˜åœ¨
        if not plan_exists and not response_exists:
            raise ValueError("Either 'plan' or 'chat_response' must be provided.")
        
        # éªŒè¯é€šè¿‡ï¼Œè¿”å› self
        return self

class MessagesState(TypedDict):
    # å®šä¹‰messageså­—æ®µï¼Œç±»å‹ä¸ºæ¶ˆæ¯åºåˆ—ï¼Œä½¿ç”¨add_messageså¤„ç†è¿½åŠ 
    messages: Annotated[Sequence[BaseMessage], add_messages]
    # å®šä¹‰planå­—æ®µï¼Œç”¨äºå­˜å‚¨é—®é¢˜çš„è®¡åˆ’
    plan: Optional[Plan]
    # ç”¨äºè·Ÿè¸ªå·²å®Œæˆçš„ä»»åŠ¡ID
    completed_tasks: List[int]
    # å®šä¹‰reflectionå­—æ®µï¼Œç”¨äºå­˜å‚¨åæ€ç»“æœ
    reflection: Optional[Reflection]      

async def tool_executor_node(state: MessagesState,* , all_tools) -> dict:
    logger.info(">>Nodeï¼šToolExcutor")
    plan = state["plan"]
    next_task = None

    for task in plan.tasks:
        if task.status == "pending":
            next_task = task
            break

    if next_task is None:
        logger.info("No pending tasks found")
        return {}
    
    tool_2_call = all_tools[next_task.tool_name]
    logger.info(f"æ‰§è¡Œä»»åŠ¡ #{next_task.task_id}ï¼Œè°ƒç”¨å·¥å…·ï¼š{tool_2_call}")

    try:
        result = await tool_2_call.ainvoke(next_task.tool_args)
        result_str = str(result)

        next_task.status = "completed"
        next_task.result = result_str

    except Exception as e:
        logger.error(f"Error executing tool {next_task.tool_name}: {e}")
        next_task.status = "failed"
        next_task.result = str(e)

    tool_message = ToolMessage(
        content=next_task.result,
        name=next_task.tool_name,
        tool_call_id=f"call_{next_task.task_id}"
    )

    completed_tasks = state.get("completed_tasks", [])
    completed_tasks.append(next_task.task_id)
    

    return {"plan": plan, "completed_tasks": completed_tasks, "messages":[tool_message]}

def format_plan_results(plan: Plan) -> str:
    formatted_string = ""
    for task in plan.tasks:
        if task.status == "completed":
            formatted_string += f"Question:{task.question}\nAnswer:{task.result}\n"
        elif task.status == "failed":
            formatted_string += f"Question:{task.question}\nAttempt failed. Error: {task.result}\n\n"

    return formatted_string

async def synthesizer_node(state: MessagesState, *, llm_chat) -> dict:
    logger.info(">>Nodeï¼šSynthesizer")

    synthesizer_chain = create_chain(llm_chat, Config.PROMPT_TEMPLATE_TXT_SYNTHESIZER)
    
    original_query = get_latest_question(state)
    plan_results = format_plan_results(state["plan"])

    fianl_answer = await synthesizer_chain.ainvoke({
        "original_query": original_query,
        "formatted_plan_results": plan_results
    })

    return {"messages": [fianl_answer]}

handler = ConcurrentRotatingFileHandler(
    # æ—¥å¿—æ–‡ä»¶
    Config.LOG_FILE,
    # æ—¥å¿—æ–‡ä»¶æœ€å¤§å…è®¸å¤§å°ä¸º5MBï¼Œè¾¾åˆ°ä¸Šé™åè§¦å‘è½®è½¬
    maxBytes = Config.MAX_BYTES,
    # åœ¨è½®è½¬æ—¶ï¼Œæœ€å¤šä¿ç•™3ä¸ªå†å²æ—¥å¿—æ–‡ä»¶
    backupCount = Config.BACKUP_COUNT
)
# è®¾ç½®å¤„ç†å™¨çº§åˆ«ä¸ºDEBUG
handler.setLevel(logging.DEBUG)
handler.setFormatter(logging.Formatter(
    "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
))
logger.addHandler(handler)




# æ–‡æ¡£ç›¸å…³æ€§è¯„åˆ†
class DocumentRelevanceScore(BaseModel):
    # å®šä¹‰binary_scoreå­—æ®µï¼Œè¡¨ç¤ºç›¸å…³æ€§è¯„åˆ†ï¼Œå–å€¼ä¸º"yes"æˆ–"no"
    binary_score: str = Field(description="Relevance score 'yes' or 'no'")

# è‡ªå®šä¹‰å¼‚å¸¸ï¼Œè¡¨ç¤ºæ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆ–çŠ¶æ€å¼‚å¸¸
class ConnectionPoolError(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ï¼Œè¡¨ç¤ºæ•°æ®åº“è¿æ¥æ± åˆå§‹åŒ–æˆ–çŠ¶æ€å¼‚å¸¸"""
    pass


# å®šä¹‰è·å–æœ€æ–°é—®é¢˜çš„è¾…åŠ©å‡½æ•°
def get_latest_question(state: MessagesState) -> Optional[str]:
    """ä»çŠ¶æ€ä¸­å®‰å…¨åœ°è·å–æœ€æ–°ç”¨æˆ·é—®é¢˜ã€‚

    Args:
        state: å½“å‰å¯¹è¯çŠ¶æ€ï¼ŒåŒ…å«æ¶ˆæ¯å†å²ã€‚

    Returns:
        Optional[str]: æœ€æ–°é—®é¢˜çš„å†…å®¹ï¼Œå¦‚æœæ— æ³•è·å–åˆ™è¿”å› Noneã€‚
    """
    try:
        # æ£€æŸ¥çŠ¶æ€æ˜¯å¦åŒ…å«æ¶ˆæ¯åˆ—è¡¨ä¸”ä¸ä¸ºç©º
        if not state.get("messages") or not isinstance(state["messages"], (list, tuple)) or len(state["messages"]) == 0:
            logger.warning("No valid messages found in state for getting latest question")
            return None

        # ä»åå‘å‰éå†æ¶ˆæ¯ï¼Œæ‰¾åˆ°æœ€è¿‘çš„ HumanMessageï¼ˆç”¨æˆ·è¾“å…¥ï¼‰
        for message in reversed(state["messages"]):
            if message.__class__.__name__ == "HumanMessage" and hasattr(message, "content"):
                return message.content

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° HumanMessageï¼Œè¿”å› None
        logger.info("No HumanMessage found in state")
        return None

    except Exception as e:
        logger.error(f"Error getting latest question: {e}")
        return None


# å®šä¹‰çº¿ç¨‹å†…çš„æŒä¹…åŒ–å­˜å‚¨æ¶ˆæ¯è¿‡æ»¤å‡½æ•°
def filter_messages(messages: list) -> list:
    """è¿‡æ»¤æ¶ˆæ¯åˆ—è¡¨ï¼Œä»…ä¿ç•™ AIMessage å’Œ HumanMessage ç±»å‹æ¶ˆæ¯"""
    # è¿‡æ»¤å‡º AIMessage å’Œ HumanMessage ç±»å‹çš„æ¶ˆæ¯
    filtered = [msg for msg in messages if msg.__class__.__name__ in ['AIMessage', 'HumanMessage']]
    # å¦‚æœè¿‡æ»¤åçš„æ¶ˆæ¯è¶…è¿‡Næ¡ï¼Œè¿”å›æœ€åNæ¡ï¼Œå¦åˆ™è¿”å›è¿‡æ»¤åçš„å®Œæ•´åˆ—è¡¨
    return filtered[-5:] if len(filtered) > 5 else filtered


# å®šä¹‰è·¨çº¿ç¨‹çš„æŒä¹…åŒ–å­˜å‚¨çš„å­˜å‚¨å’Œè¿‡æ»¤å‡½æ•°
async def store_memory(question: BaseMessage, config: RunnableConfig, store: BaseStore) -> str:
    """å­˜å‚¨ç”¨æˆ·è¾“å…¥ä¸­çš„è®°å¿†ä¿¡æ¯ã€‚

    Args:
        question: ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯ã€‚
        config: è¿è¡Œæ—¶é…ç½®ã€‚
        store: æ•°æ®å­˜å‚¨å®ä¾‹ã€‚

    Returns:
        str: ç”¨æˆ·ç›¸å…³çš„è®°å¿†ä¿¡æ¯å­—ç¬¦ä¸²ã€‚
    """
    # åœ¨ store_memory å‡½æ•°å¼€å¤´é™„è¿‘
    logger.debug(f"store_memory called with question content: {repr(question.content)}, type: {type(question.content)}")
    namespace = ("memories", config["configurable"]["user_id"])
    try:
                # ç¡®ä¿æŸ¥è¯¢å†…å®¹æ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²
        query_content = ""
        if hasattr(question, 'content') and question.content:
            if isinstance(question.content, str):
                query_content = question.content.strip()
            elif isinstance(question.content, (list, tuple)):
                # å¦‚æœæ˜¯åˆ—è¡¨æˆ–å…ƒç»„ï¼Œæå–å­—ç¬¦ä¸²å†…å®¹
                query_content = " ".join([str(item) for item in question.content if item])
            else:
                query_content = str(question.content).strip()
        
        # éªŒè¯æŸ¥è¯¢å†…å®¹ä¸ä¸ºç©ºä¸”æ˜¯æœ‰æ•ˆå­—ç¬¦ä¸²
        if not query_content or len(query_content.strip()) == 0:
            logger.warning("Empty or invalid query content, skipping memory search")
            return ""
        
        # é™åˆ¶æŸ¥è¯¢é•¿åº¦ï¼Œé¿å…APIé™åˆ¶
        if len(query_content) > 1000:
            query_content = query_content[:1000]
            logger.debug(f"Truncated query content to 1000 characters")
        
        logger.debug(f"Searching memories with query: {repr(query_content)}")
        # åœ¨è·¨çº¿ç¨‹å­˜å‚¨æ•°æ®åº“ä¸­æœç´¢ç›¸å…³è®°å¿†
        memories = await store.asearch(namespace, query=query_content)
        user_info = "\n".join([d.value["data"] for d in memories])

        # å¦‚æœåŒ…å«â€œè®°ä½â€ï¼Œå­˜å‚¨æ–°è®°å¿†
        if "è®°ä½" in query_content.lower():
            memory = escape(query_content)
            await store.aput(namespace, str(uuid.uuid4()), {"data": memory})
            logger.info(f"Stored memory: {memory}")

        return user_info
    except Exception as e:
        logger.error(f"Error in store_memory: {e}")
        return ""


def format_examples_for_prompt(examples: list) -> str:
    """
    å°† few-shot ç¤ºä¾‹åˆ—è¡¨æ ¼å¼åŒ–ä¸ºå•ä¸ªå­—ç¬¦ä¸²ï¼Œä»¥ä¾¿æ³¨å…¥åˆ° prompt æ¨¡æ¿ä¸­ã€‚
    """
    if not examples:
        return "" # å¦‚æœæ²¡æœ‰ç¤ºä¾‹ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    
    formatted_str = ""
    for example_pair in examples:
        # å‡è®¾ example_pair æ˜¯ ("human", "...") æˆ– ("ai", "...") çš„å…ƒç»„
        # æˆ‘ä»¬å¯ä»¥æ ¹æ®éœ€è¦æ ¼å¼åŒ–å®ƒ
        # è¿™é‡Œé‡‡ç”¨ä¸€ç§ç±»ä¼¼ Markdown çš„æ ¼å¼
        if example_pair[0] == "human":
            formatted_str += f"*   **User Query**: \"{example_pair[1]}\"\n"
        elif example_pair[0] == "ai":
            formatted_str += f"*   **Your Output**:\n    ```json\n{example_pair[1].strip()}\n    ```\n\n"

    
            
    return formatted_str.strip()

# æˆ‘ä»¬æš‚æ—¶ä¸è€ƒè™‘ç¼“å­˜ï¼Œä¸“æ³¨äºæ ¸å¿ƒé€»è¾‘
def create_chain(
    llm_chat: Any,
    template_file: str, 
    structured_output: Optional[Any] = None
):
    """
    ä»æ–‡ä»¶åŠ è½½ä¸€ä¸ªæ¨¡æ¿å¹¶åˆ›å»ºä¸€ä¸ª LLM Chainã€‚
    æ¨¡æ¿ä¸­çš„æ‰€æœ‰å˜é‡éƒ½æœŸæœ›åœ¨ .ainvoke() æ—¶è¢«æä¾›ã€‚
    """
    try:
        # 1. ç›´æ¥ä½¿ç”¨ PromptTemplate.from_file åŠ è½½æ¨¡æ¿ã€‚
        # LangChain ä¼šè‡ªåŠ¨è¯†åˆ« {examples}, {query}, {history} ç­‰å ä½ç¬¦ã€‚
        prompt = PromptTemplate.from_file(template_file, encoding="utf-8")
        # 2. è½¬æ¢ä¸º ChatPromptTemplate å¹¶ç»„è£… Chain
        # æˆ‘ä»¬å‡è®¾æ•´ä¸ªæ¨¡æ¿éƒ½æ˜¯ human message çš„ä¸€éƒ¨åˆ†
        chat_prompt = ChatPromptTemplate.from_messages([
            ("human", prompt.template)
        ])
        
        return chat_prompt | (llm_chat.with_structured_output(structured_output) if structured_output else llm_chat)
    except Exception as e:
            logger.error(f"Error getting latest question: {e}")
            return None


# æ•°æ®åº“é‡è¯•æœºåˆ¶,æœ€å¤šé‡è¯•3æ¬¡,æŒ‡æ•°é€€é¿ç­‰å¾…2-10ç§’,ä»…å¯¹æ•°æ®åº“æ“ä½œé”™è¯¯é‡è¯•
@retry(stop=stop_after_attempt(3),wait=wait_exponential(multiplier=1, min=2, max=10),retry=retry_if_exception_type(OperationalError))
def test_connection(db_connection_pool: ConnectionPool) -> bool:
    """æµ‹è¯•è¿æ¥æ± æ˜¯å¦å¯ç”¨"""
    with db_connection_pool.getconn() as conn:
        with conn.cursor() as cursor:
            cursor.execute("SELECT 1")
            result = cursor.fetchone()
            if result != (1,):
                raise ConnectionPoolError("è¿æ¥æ± æµ‹è¯•æŸ¥è¯¢å¤±è´¥ï¼Œè¿”å›ç»“æœå¼‚å¸¸")
    return True


# å‘¨æœŸæ€§æ£€æŸ¥è¿æ¥æ± çŠ¶æ€ï¼Œè®°å½•å¯ç”¨è¿æ¥æ•°å’Œå¼‚å¸¸æƒ…å†µï¼Œæå‰é¢„è­¦
def monitor_connection_pool(db_connection_pool: ConnectionPool, interval: int = 60):
    """å‘¨æœŸæ€§ç›‘æ§è¿æ¥æ± çŠ¶æ€"""
    def _monitor():
        while not db_connection_pool.closed:
            try:
                stats = db_connection_pool.get_stats()
                active = stats.get("connections_in_use", 0)
                total = db_connection_pool.max_size
                logger.info(f"Connection db_connection_pool status: {active}/{total} connections in use")
                if active >= total * 0.8:
                    logger.warning(f"Connection db_connection_pool nearing capacity: {active}/{total}")
            except Exception as e:
                logger.error(f"Failed to monitor connection db_connection_pool: {e}")
            time.sleep(interval)

    monitor_thread = threading.Thread(target=_monitor, daemon=True)
    monitor_thread.start()
    return monitor_thread


async def planner_agent(state: MessagesState, config: RunnableConfig, *, store: BaseStore, llm_chat) -> dict:
    """ä»£ç†å‡½æ•°ï¼Œæ ¹æ®ç”¨æˆ·é—®é¢˜å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·æˆ–ç»“æŸã€‚

    Args:
        state: å½“å‰å¯¹è¯çŠ¶æ€ã€‚
        config: è¿è¡Œæ—¶é…ç½®ã€‚
        store: æ•°æ®å­˜å‚¨å®ä¾‹ã€‚
        llm_chat: Chatæ¨¡å‹ã€‚
        tool_config: å·¥å…·é…ç½®å‚æ•°ã€‚

    Returns:
        dict: æ›´æ–°åçš„å¯¹è¯çŠ¶æ€ã€‚
    """
    # è®°å½•ä»£ç†å¼€å§‹å¤„ç†æŸ¥è¯¢
    logger.info("Planner Agent processing user query")
    # å®šä¹‰å­˜å‚¨å‘½åç©ºé—´ï¼Œä½¿ç”¨ç”¨æˆ·ID
    namespace = ("memories", config["configurable"]["user_id"])
    # å°è¯•æ‰§è¡Œä»¥ä¸‹ä»£ç å—
    
    try:
        # è·å–æœ€åä¸€æ¡æ¶ˆæ¯å³ç”¨æˆ·é—®é¢˜
        question = state["messages"][-1]
        # è‡ªå®šä¹‰çº¿ç¨‹å†…å­˜å‚¨é€»è¾‘ è¿‡æ»¤æ¶ˆæ¯
        messages = filter_messages(state["messages"])
        logger.info(f"agent question:{question}")
        # åœ¨ agent å‡½æ•°ä¸­ï¼Œè·å– question ä¹‹å
        logger.debug(f"Processing question message: {type(question)}, content type: {type(question.content)}")
        # è‡ªå®šä¹‰è·¨çº¿ç¨‹æŒä¹…åŒ–å­˜å‚¨è®°å¿†å¹¶è·å–ç›¸å…³ä¿¡æ¯
        user_info = await store_memory(question, config, store)
        examples_str = format_examples_for_prompt(PLANNER_EXAMPLES)
        # examples_chat_response_str = format_examples_for_prompt(PLANNER_EXAMPLES_CHAT_RESPONSE)
        # åˆ›å»ºä»£ç†å¤„ç†é“¾
        agent_chain = create_chain(llm_chat, Config.PLANNER_AGENT_PROMPT)
        # è°ƒç”¨ä»£ç†é“¾å¤„ç†æ¶ˆæ¯
        responses = await agent_chain.ainvoke({"query": question,"history": messages, "examples": examples_str})
        logger.info(f"Planner Agent response: {responses}")

        llm_output_str = responses.content
        logger.info(f"LLM Raw Output String:\n---\n{llm_output_str}\n---")

        # --- è°ƒè¯•æ­¥éª¤ 2: æ‰‹åŠ¨æ‰§è¡Œè§£æå’ŒéªŒè¯ ---
        response = None
        try:
            # a. ä» Markdown ä¸­æå– JSON
            import re
            import json
            from pydantic import ValidationError
            match = re.search(r"```(json)?\n(.*)```", llm_output_str, re.DOTALL)
            if match:
                print("it has markdown code")
                json_str = match.group(2)
            else:
                print("it has no markdown code")
                json_str = llm_output_str
            
            logger.info(f"Extracted JSON String:\n---\n{json_str}\n---")
            
            # b. è§£æ JSON å­—ç¬¦ä¸²
            data_dict = json.loads(json_str)
            
            # c. éªŒè¯ Pydantic æ¨¡å‹
            response = PlannerOutput(**data_dict)
            
            logger.info("Manual parsing and validation successful!")

        except json.JSONDecodeError as e:
            logger.error(f"Manual JSON Decode Failed: {e}")
        except ValidationError as e:
            logger.error(f"Manual Pydantic Validation Failed: {e}")
        except Exception as e:
            logger.error(f"An unexpected error occurred during manual parsing: {e}")
        if response is None:
            logger.error("LLM failed to return a valid PlannerOutput object. It returned None.")
            # å½“è§£æå¤±è´¥æ—¶ï¼Œæˆ‘ä»¬æ— æ³•çŸ¥é“ç”¨æˆ·çš„æ„å›¾ï¼Œæœ€å¥½çš„åšæ³•æ˜¯å‘ç”¨æˆ·è¯·æ±‚æ¾„æ¸…
            error_message = AIMessage(content="I'm sorry, I'm having trouble understanding your request. Could you please rephrase it?")
            print("\n" + "="*20 + " æœ€ç»ˆç­”æ¡ˆ " + "="*20)
            print(f"ğŸ¤– **Assistant**: {error_message.content}")
            print("="*50 + "\n")
            # è¿”å›ä¸€ä¸ªæ˜ç¡®çš„â€œæ— è®¡åˆ’â€çŠ¶æ€
            return {**state, "messages": state["messages"] + [error_message], "plan": None}
        
        if response.chat_response:
            print("make a response")
            logger.info(f"Planner decided to chat directly. Response: {response.chat_response}")
            # æˆ‘ä»¬è¿”å›è¿™ä¸ªæ¶ˆæ¯ï¼Œå¹¶ä¸” plan ä¸º Noneï¼Œè¿™å°†å¯¼è‡´å›¾ç›´æ¥ç»“æŸ
            return {**state, "messages": [response.chat_response], "plan": None}
        elif response.plan:
            print("make a plan")
            logger.info(f"Planner created a new plan. Thought: {response.plan.thought}")
            # è¿”å›æ–°çš„è®¡åˆ’ï¼Œå¹¶æ¸…ç©º completed_tasks
            return {**state, "plan": response.plan, "completed_tasks": []}

        # å…œåº•æƒ…å†µï¼Œè™½ç„¶ Pydantic validator åº”è¯¥èƒ½é˜»æ­¢è¿™ç§æƒ…å†µ
        else:
            logger.error("Planner output was invalid (neither plan nor chat_response).")
            # è¿”å›ä¸€ä¸ªé”™è¯¯æ¶ˆæ¯
            error_message = AIMessage(content="I'm sorry, I had trouble understanding that. Could you please rephrase?")
            return {**state, "messages": [error_message], "plan": None}

    except Exception as e:
        logger.error(f"Error in agent processing: {e}", exc_info=True)
        return {"messages": [{"role": "system", "content": "å¤„ç†è¯·æ±‚æ—¶å‡ºé”™"}]}


async def reflector_node(state: MessagesState, *, llm_chat) -> dict:
    logger.info(">>Nodeï¼šReflector")
    try:
        plan = state.get("plan")
        completed_tasks = state.get("completed_tasks", [])

        if not completed_tasks:
            return {"reflection": Reflection(assessment="success", reasoning="No tasks to reflect on.")}

        last_completed_id = completed_tasks[-1]
        last_task = next((task for task in plan.tasks if task.task_id == last_completed_id), None)

        if not last_task:
            logger.error(f"FATAL: Cannot find task with ID {last_completed_id} in the plan.")
            # ä¸¥é‡é”™è¯¯ï¼Œå¼ºåˆ¶å¤±è´¥
            return {"reflection": Reflection(assessment="failure", reasoning=f"Task ID {last_completed_id} not found in plan.")}
        logger.info(f"Reflecting on Task {last_task.task_id}: '{last_task.question}'")
        logger.info(f"Tool Result: '{str(last_task.result)[:500]}...'")

        reflector_chain = create_chain(llm_chat, Config.REFLECTOR_AGENT_PROMPT)
        reflection_result: Reflection = await reflector_chain.ainvoke({
            "question": last_task.question,
            "result": str(last_task.result)
        })
        llm_output_str = reflection_result.content
        # --- è°ƒè¯•æ­¥éª¤ 2: æ‰‹åŠ¨æ‰§è¡Œè§£æå’ŒéªŒè¯ ---
        response = None
        try:
            # a. ä» Markdown ä¸­æå– JSON
            import re
            import json
            from pydantic import ValidationError
            match = re.search(r"```(json)?\n(.*)```", llm_output_str, re.DOTALL)
            if match:
                print("it has markdown code")
                json_str = match.group(2)
            else:
                print("it has no markdown code")
                json_str = llm_output_str
            
            logger.info(f"Extracted JSON String:\n---\n{json_str}\n---")
            
            # b. è§£æ JSON å­—ç¬¦ä¸²
            data_dict = json.loads(json_str)
            
            # c. éªŒè¯ Pydantic æ¨¡å‹
            response = Reflection(**data_dict)
            
            logger.info("Manual parsing and validation successful!")

        except json.JSONDecodeError as e:
            logger.error(f"Manual JSON Decode Failed: {e}")
        except ValidationError as e:
            logger.error(f"Manual Pydantic Validation Failed: {e}")
        except Exception as e:
            logger.error(f"An unexpected error occurred during manual parsing: {e}")
        reflection_result = response
        logger.info(f"Reflection Result: {reflection_result}")
        logger.info(f"Reflection Assessment: {reflection_result.assessment} | Reasoning: {reflection_result.reasoning}")
        return {"reflection": reflection_result}
    except (IndexError, KeyError) as e:
        logger.error(f"Message access error: {e}")
        return {
            "messages": [{"role": "system", "content": "æ— æ³•è¯„åˆ¤ä»»åŠ¡"}],
            "reflection": None
        }
    except Exception as e:
        logger.error(f"Unexpected error in grading: {e}")
        return {
            "messages": [{"role": "system", "content": "è¯„åˆ¤ä»»åŠ¡è¿‡ç¨‹ä¸­å‡ºé”™"}],
            "reflection": None
        }
        


async def replanner_node(state: MessagesState, *,llm_chat) -> dict:
    logger.info(">>Nodeï¼šReplanner")
    try:
        last_reflection = state.get("reflection")
        original_plan = state.get("plan")
        completed_tasks = state.get("completed_tasks", [])

        if not last_reflection or not original_plan:
            logger.error("RePlanner called without reflection or plan. Aborting.")
            return {}

        last_completed_id = completed_tasks[-1] if completed_tasks else 0
        failed_task = next((task for task in original_plan.tasks if task.task_id == last_completed_id), original_plan.tasks[0])
        # æ ¼å¼åŒ–åŸå§‹è®¡åˆ’ï¼ŒåŒ…å«å·²æˆåŠŸçš„ç»“æœ
        plan_with_results_str = ""
        for task in original_plan.tasks:
            plan_with_results_str += f"Task {task.task_id}: {task.question}\n"
            if task.task_id in completed_tasks:
                plan_with_results_str += f"  Status: Success\n  Result: {task.result}\n"
            elif task.task_id == failed_task.task_id:
                plan_with_results_str += f"  Status: Failed\n"
            else:
                plan_with_results_str += f"  Status: Pending\n"
        
        replanner_chain = create_chain(llm_chat, Config.REPLANNER_AGENT_PROMPT)

        new_plan = await replanner_chain.ainvoke({
            "original_plan_with_results": plan_with_results_str,
            "failed_task_id": failed_task.task_id,
            "failed_task_question": failed_task.question,
            "failure_reason": last_reflection.reasoning,
            "suggestion": last_reflection.suggestion_for_next_step
        })
        llm_output_str = new_plan.content
        response = None
        try:
            # a. ä» Markdown ä¸­æå– JSON
            import re
            import json
            from pydantic import ValidationError
            match = re.search(r"```(json)?\n(.*)```", llm_output_str, re.DOTALL)
            if match:
                print("it has markdown code")
                json_str = match.group(2)
            else:
                print("it has no markdown code")
                json_str = llm_output_str
            
            logger.info(f"Extracted JSON String:\n---\n{json_str}\n---")
            
            # b. è§£æ JSON å­—ç¬¦ä¸²
            data_dict = json.loads(json_str)
            
            # c. éªŒè¯ Pydantic æ¨¡å‹
            response = PlannerOutput(**data_dict)
            
            logger.info("Manual parsing and validation successful!")

        except json.JSONDecodeError as e:
            logger.error(f"Manual JSON Decode Failed: {e}")
        except ValidationError as e:
            logger.error(f"Manual Pydantic Validation Failed: {e}")
        except Exception as e:
            logger.error(f"An unexpected error occurred during manual parsing: {e}")
        new_plan = response
        logger.info(f"Replanning complete. New plan has {len(new_plan.plan.tasks)} tasks.")
        logger.info(f"New plan: {new_plan.plan}")
        return {"plan": new_plan.plan}
    except (IndexError, KeyError) as e:
        # è®°å½•é”™è¯¯æ—¥å¿—
        logger.error(f"Message access error in replanner: {e}")
        # è¿”å›é”™è¯¯æ¶ˆæ¯
        return {"messages": [{"role": "system", "content": "æ— æ³•é‡å†™æŸ¥è¯¢"}]}
        



    
def decide_next_step(state: MessagesState) -> Literal["end", "continue", "replanner"]:
    reflection = state.get("reflection")
    if reflection and reflection.assessment == "failure":
        logger.info("Reflection indicates failure. Routing to RePlanner.")
        return "replanner"

    plan = state.get("plan")
    completed_tasks = state.get("completed_tasks", [])
    if not plan or len(completed_tasks) == len(plan.tasks):
        logger.info("Plan completed or does not exist. Routing to end.")
        return "end"
    else:
        logger.info("Plan not completed. Routing to continue.")
        return "continue"
    
def decide_after_planner(state: MessagesState) -> Literal["end", "call_tool"]:
    if state.get("plan") and state["plan"].tasks:
        logger.info("Plan exists and has tasks. Routing to call_tool.")
        return "call_tool"
    else:
        logger.info("Plan does not exist or has no tasks. Routing to end.")
        return "end"
    


# ä¿å­˜çŠ¶æ€å›¾çš„å¯è§†åŒ–è¡¨ç¤º
def save_graph_visualization(graph: StateGraph, filename: str = "graph.png") -> None:
    """ä¿å­˜çŠ¶æ€å›¾çš„å¯è§†åŒ–è¡¨ç¤ºã€‚

    Args:
        graph: çŠ¶æ€å›¾å®ä¾‹ã€‚
        filename: ä¿å­˜æ–‡ä»¶è·¯å¾„ã€‚
    """
    # å°è¯•æ‰§è¡Œä»¥ä¸‹ä»£ç å—
    try:
        # ä»¥äºŒè¿›åˆ¶å†™æ¨¡å¼æ‰“å¼€æ–‡ä»¶
        with open(filename, "wb") as f:
            # å°†çŠ¶æ€å›¾è½¬æ¢ä¸ºMermaidæ ¼å¼çš„PNGå¹¶å†™å…¥æ–‡ä»¶
            f.write(graph.get_graph().draw_mermaid_png())
        # è®°å½•ä¿å­˜æˆåŠŸçš„æ—¥å¿—
        logger.info(f"Graph visualization saved as {filename}")
    # æ•è·IOé”™è¯¯
    except IOError as e:
        # è®°å½•è­¦å‘Šæ—¥å¿—
        logger.warning(f"Failed to save graph visualization: {e}")


# åˆ›å»ºå¹¶é…ç½®çŠ¶æ€å›¾
def create_graph(db_connection_pool: ConnectionPool, llm_chat, llm_embedding, all_tools) -> StateGraph:
    """åˆ›å»ºå¹¶é…ç½®çŠ¶æ€å›¾ã€‚

    Args:
        db_connection_pool: æ•°æ®åº“è¿æ¥æ± ã€‚
        llm_chat: Chatæ¨¡å‹ã€‚
        llm_embedding: Embeddingæ¨¡å‹ã€‚
        all_tools: å·¥å…·é…ç½®å­—å…¸ã€‚

    Returns:
        StateGraph: ç¼–è¯‘åçš„çŠ¶æ€å›¾ã€‚

    Raises:
        ConnectionPoolError: å¦‚æœè¿æ¥æ± æœªæ­£ç¡®åˆå§‹åŒ–æˆ–çŠ¶æ€å¼‚å¸¸ã€‚
    """
    if db_connection_pool is None or db_connection_pool.closed:
        logger.error("Connection db_connection_pool is None or closed")
        raise ConnectionPoolError("æ•°æ®åº“è¿æ¥æ± æœªåˆå§‹åŒ–æˆ–å·²å…³é—­")
    try:
        # è·å–å½“å‰æ´»åŠ¨è¿æ¥æ•°å’Œæœ€å¤§è¿æ¥æ•°
        active_connections = db_connection_pool.get_stats().get("connections_in_use", 0)
        max_connections = db_connection_pool.max_size
        if active_connections >= max_connections:
            logger.error(f"Connection db_connection_pool exhausted: {active_connections}/{max_connections} connections in use")
            raise ConnectionPoolError("è¿æ¥æ± å·²è€—å°½ï¼Œæ— å¯ç”¨è¿æ¥")
        if not test_connection(db_connection_pool):
            raise ConnectionPoolError("è¿æ¥æ± æµ‹è¯•å¤±è´¥")
        logger.info("Connection db_connection_pool status: OK, test connection successful")
    except OperationalError as e:
        logger.error(f"Database operational error during connection test: {e}")
        raise ConnectionPoolError(f"è¿æ¥æ± æµ‹è¯•å¤±è´¥ï¼Œå¯èƒ½å·²å…³é—­æˆ–è¶…æ—¶: {str(e)}")
    except Exception as e:
        logger.error(f"Failed to verify connection db_connection_pool status: {e}")
        raise ConnectionPoolError(f"æ— æ³•éªŒè¯è¿æ¥æ± çŠ¶æ€: {str(e)}")

    # çº¿ç¨‹å†…æŒä¹…åŒ–å­˜å‚¨
    try:
        # åˆ›å»ºPostgresæ£€æŸ¥ç‚¹ä¿å­˜å®ä¾‹
        checkpointer = PostgresSaver(db_connection_pool)
        # åˆå§‹åŒ–æ£€æŸ¥ç‚¹
        checkpointer.setup()
    except Exception as e:
        logger.error(f"Failed to setup PostgresSaver: {e}")
        raise ConnectionPoolError(f"æ£€æŸ¥ç‚¹åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    # try:
    # # ä¸´æ—¶ä½¿ç”¨å†…å­˜ Checkpointer
    #     from langgraph.checkpoint.memory import MemorySaver
    #     checkpointer = MemorySaver()
    #     logger.info("ä½¿ç”¨å†…å­˜ Checkpointerï¼ˆå¼€å‘/æµ‹è¯•æ¨¡å¼ï¼‰")
    # except Exception as e:
    #     logger.error(f"Failed to setup MemorySaver: {e}")
    #     raise ConnectionPoolError(f"æ£€æŸ¥ç‚¹åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    # è·¨çº¿ç¨‹æŒä¹…åŒ–å­˜å‚¨
    try:
        # åˆ›å»ºPostgreså­˜å‚¨å®ä¾‹ï¼ŒæŒ‡å®šåµŒå…¥ç»´åº¦å’Œå‡½æ•°
        store = PostgresStore(db_connection_pool, index={"dims": 1024, "embed": llm_embedding})
        store.setup()
    except Exception as e:
        logger.error(f"Failed to setup PostgresStore: {e}")
        raise ConnectionPoolError(f"å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    workflow = StateGraph(MessagesState)
    # æ·»åŠ ä»£ç†èŠ‚ç‚¹
    agent_with_args = functools.partial(
                                planner_agent, 
                                store=store, 
                                llm_chat=llm_chat, 
                            )
    tool_executor_node_with_args = functools.partial(
                                tool_executor_node,
                                all_tools=all_tools
                            )
    synthesizer_node_with_args = functools.partial(
                                synthesizer_node,
                                llm_chat=llm_chat
                            )
    reflector_node_with_args = functools.partial(
                                reflector_node,
                                llm_chat=llm_chat
                            )
    replanner_node_with_args = functools.partial(
                                replanner_node,
                                llm_chat=llm_chat
                            )
    
    workflow.add_node("planner_agent", agent_with_args)
    # æ·»åŠ å·¥å…·èŠ‚ç‚¹ï¼Œä½¿ç”¨å¹¶è¡Œå·¥å…·èŠ‚ç‚¹
    workflow.add_node("call_tools", tool_executor_node_with_args)
    # æ·»åŠ é‡å†™èŠ‚ç‚¹
    workflow.add_node("replanner", replanner_node_with_args)
    # æ·»åŠ ç”ŸæˆèŠ‚ç‚¹
    workflow.add_node("synthesizer", synthesizer_node_with_args)
    # æ·»åŠ æ–‡æ¡£ç›¸å…³æ€§è¯„åˆ†èŠ‚ç‚¹
    workflow.add_node("reflector", reflector_node_with_args)

    # æ·»åŠ ä»èµ·å§‹åˆ°ä»£ç†çš„è¾¹
    workflow.add_edge(START, end_key="planner_agent")
    # æ·»åŠ ä»£ç†çš„æ¡ä»¶è¾¹ï¼Œæ ¹æ®å·¥å…·è°ƒç”¨çš„å·¥å…·åç§°å†³å®šä¸‹ä¸€æ­¥è·¯ç”±
    workflow.add_conditional_edges(source="planner_agent", path=decide_after_planner, path_map={"end": END, "call_tool": "call_tools"})
    # æ·»åŠ æ£€ç´¢çš„æ¡ä»¶è¾¹ï¼Œæ ¹æ®å·¥å…·è°ƒç”¨çš„ç»“æœåŠ¨æ€å†³å®šä¸‹ä¸€æ­¥è·¯ç”±
    workflow.add_edge(start_key="call_tools", end_key="reflector")
    # æ·»åŠ æ£€ç´¢çš„æ¡ä»¶è¾¹ï¼Œæ ¹æ®çŠ¶æ€ä¸­çš„è¯„åˆ†ç»“æœå†³å®šä¸‹ä¸€æ­¥è·¯ç”±
    workflow.add_conditional_edges(source="reflector", path=decide_next_step, path_map={"end": "synthesizer", "replanner": "replanner","continue": "call_tools"})
    # æ·»åŠ ä»ç”Ÿæˆåˆ°ç»“æŸçš„è¾¹
    workflow.add_edge(start_key="synthesizer", end_key=END)
    # æ·»åŠ ä»é‡å†™åˆ°ä»£ç†çš„è¾¹
    workflow.add_edge(start_key="replanner", end_key="call_tools")

    # ç¼–è¯‘çŠ¶æ€å›¾ï¼Œç»‘å®šæ£€æŸ¥ç‚¹å’Œå­˜å‚¨
    return workflow.compile(checkpointer=checkpointer, store=store)


# å®šä¹‰å“åº”å‡½æ•°
async def graph_response(graph: StateGraph, user_input: str, config: dict) -> None:
    """
    ä¸€ä¸ªé€‚é…è§„åˆ’å‹ Agent çš„å“åº”å‡½æ•°ã€‚
    å®ƒä¼šåƒè§£è¯´ä¸€æ ·ï¼Œå®æ—¶å±•ç¤º Agent çš„æ€è€ƒã€è®¡åˆ’ã€è¡ŒåŠ¨å’Œæœ€ç»ˆç­”æ¡ˆã€‚
    """
    logger.info("="*50)
    logger.info(f"å¼€å§‹å¤„ç†æ–°æŸ¥è¯¢: '{user_input}'")
    logger.info("="*50)

    try:
        # åˆå§‹çŠ¶æ€ï¼Œæ˜ç¡®æ‰€æœ‰å­—æ®µ
        initial_state = {
            "messages": [("user", user_input)],
            "plan": None,
            "completed_tasks": [],
            "reflection": None,
        }
        
        # å¯åŠ¨äº‹ä»¶æµ
        events = graph.astream(initial_state, config)
        
        # éå†äº‹ä»¶æµ
        async for event in events:
            # event çš„é”®å°±æ˜¯å½“å‰æ‰§è¡Œå®Œæ¯•çš„èŠ‚ç‚¹å
            node_name = list(event.keys())[0]
            state_update = event[node_name]

            # --- åƒè§£è¯´å‘˜ä¸€æ ·ï¼Œæ ¹æ®èŠ‚ç‚¹åæ‰“å°ä¸åŒçš„ä¿¡æ¯ ---

            if node_name == "planner":
                plan = state_update.get("plan")
                if plan and plan.tasks:
                    print(f"ğŸ¤” **æ€è€ƒä¸è§„åˆ’ä¸­...**")
                    print(f"   - æ ¸å¿ƒæ€è·¯: {plan.thought}")
                    print(f"   - åˆ¶å®šäº† {len(plan.tasks)} ä¸ªæ­¥éª¤çš„è®¡åˆ’ï¼š")
                    for task in plan.tasks:
                        print(f"     - æ­¥éª¤ {task.task_id}: ä½¿ç”¨å·¥å…· `{task.tool_name}` æ¥å›ç­” '{task.question}'")

            elif node_name == "tool_executor":
                # ä» state_update ä¸­è·å–æœ€æ–°çš„ plan å’Œ messages
                plan = state_update.get("plan")
                last_message = state_update.get("messages", [])[-1] if state_update.get("messages") else None

                if plan and isinstance(last_message, ToolMessage):
                    # æ‰¾åˆ°åˆšåˆšæ‰§è¡Œçš„ä»»åŠ¡
                    last_completed_id = state_update.get("completed_tasks", [])[-1]
                    completed_task = next((t for t in plan.tasks if t.task_id == last_completed_id), None)
                    
                    if completed_task:
                        print(f"ğŸ› ï¸ **æ‰§è¡Œä»»åŠ¡ä¸­...** (æ­¥éª¤ {completed_task.task_id}/{len(plan.tasks)})")
                        print(f"   - è°ƒç”¨å·¥å…·: `{completed_task.tool_name}`")
                        print(f"   - å·¥å…·å‚æ•°: {completed_task.tool_args}")
                        # æ‰“å°éƒ¨åˆ†ç»“æœï¼Œé¿å…åˆ·å±
                        print(f"   - è·å¾—ç»“æœ: '{completed_task.result[:150]}...'")

            elif node_name == "reflector":
                reflection = state_update.get("reflection")
                if reflection:
                    if reflection.assessment == "success":
                        print(f"âœ… **åæ€ç»“æœ: æˆåŠŸ**")
                        print(f"   - è¯„ä¼°: å·¥å…·ç»“æœæœ‰æ•ˆã€‚")
                    else:
                        print(f"âŒ **åæ€ç»“æœ: å¤±è´¥**")
                        print(f"   - è¯„ä¼°: {reflection.reasoning}")
                        if reflection.suggestion_for_next_step:
                             print(f"   - å»ºè®®: {reflection.suggestion_for_next_step}")

            elif node_name == "replanner":
                new_plan = state_update.get("plan")
                if new_plan:
                    print(f"ğŸ”„ **è°ƒæ•´è®¡åˆ’ä¸­...**")
                    print(f"   - åŸå§‹è®¡åˆ’å­˜åœ¨é—®é¢˜ï¼Œæ­£åœ¨ç”Ÿæˆæ–°è®¡åˆ’...")
                    # å¯ä»¥åœ¨è¿™é‡Œæ‰“å°æ–°è®¡åˆ’çš„ thought

            elif node_name == "synthesizer":
                final_answer_message = state_update.get("messages", [])[-1] if state_update.get("messages") else None
                if final_answer_message:
                    print("\n" + "="*20 + " æœ€ç»ˆç­”æ¡ˆ " + "="*20)
                    print(f"ğŸ¤– **Assistant**: {final_answer_message.content}")
                    print("="*50 + "\n")
            
            # å¯ä»¥åœ¨è¿™é‡ŒåŠ ä¸€ä¸ª pprint(event) æ¥è¿›è¡Œæ·±åº¦è°ƒè¯•
            # from pprint import pprint
            # pprint(event)

    except Exception as e:
        logger.error(f"å¤„ç†å“åº”æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}", exc_info=True)
        print("\nâŒ **é”™è¯¯**: å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‘ç”Ÿäº†ä¸€ä¸ªå†…éƒ¨é”™è¯¯ã€‚è¯·ç¨åå†è¯•ã€‚")

# å®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥å¼‚æ­¥è·å–è¾“å…¥
async def ainput(prompt: str = ""):
    return await asyncio.get_event_loop().run_in_executor(
        None, lambda: input(prompt)
    )
# å®šä¹‰ä¸»å‡½æ•°
async def main():
    """ä¸»å‡½æ•°ï¼Œåˆå§‹åŒ–å¹¶è¿è¡ŒèŠå¤©æœºå™¨äººã€‚"""
    # åˆå§‹åŒ–è¿æ¥æ± ä¸ºNone
    db_connection_pool = None
    try:
        # è°ƒç”¨get_llmå‡½æ•°åˆå§‹åŒ–Chatæ¨¡å‹å®ä¾‹å’ŒEmbeddingæ¨¡å‹å®ä¾‹
        llm_chat, llm_embedding = get_llm(Config.LLM_TYPE)
        client = MultiServerMCPClient(
            {
                "finMCPServer": {
                    "command": "python",
                    # Make sure to update to the full absolute path to your math_server.py file
                    "args": ["finMCPServer.py"],
                    "transport": "stdio",
                }
            }
        )

        # è·å–å·¥å…·åˆ—è¡¨
        # tools = get_tools(llm_embedding)
        tools = await client.get_tools()
        all_tools = {tool.name: tool for tool in tools}


        # å®šä¹‰æ•°æ®åº“è¿æ¥å‚æ•°ï¼Œè‡ªåŠ¨æäº¤ä¸”æ— é¢„å‡†å¤‡é˜ˆå€¼ï¼Œ5ç§’è¶…æ—¶
        connection_kwargs = {"autocommit": True, "prepare_threshold": 0, "connect_timeout": 5}
        db_connection_pool = ConnectionPool(conninfo=Config.DB_URI, max_size=20, min_size=2, kwargs=connection_kwargs, timeout=10)
        # åˆ›å»ºçŠ¶æ€å›¾
        try:
            graph = create_graph(db_connection_pool, llm_chat, llm_embedding, all_tools)
        except ConnectionPoolError as e:
            logger.error(f"Graph creation failed: {e}")
            print(f"é”™è¯¯: {e}")
            sys.exit(1)


        # ä¿å­˜çŠ¶æ€å›¾å¯è§†åŒ–
        save_graph_visualization(graph)

        # æ‰“å°æœºå™¨äººå°±ç»ªæç¤º
        print("èŠå¤©æœºå™¨äººå‡†å¤‡å°±ç»ªï¼è¾“å…¥ 'quit'ã€'exit' æˆ– 'q' ç»“æŸå¯¹è¯ã€‚")
        # å®šä¹‰è¿è¡Œæ—¶é…ç½®ï¼ŒåŒ…å«çº¿ç¨‹IDå’Œç”¨æˆ·ID
        config = {"configurable": {"thread_id": "1", "user_id": "1"}}
        # è¿›å…¥ä¸»å¾ªç¯
        while True:
            # è·å–ç”¨æˆ·è¾“å…¥å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
            #user_input = input("User: ").strip()
            user_input = (await ainput("User: ")).strip()
            # æ£€æŸ¥æ˜¯å¦é€€å‡º
            if user_input.lower() in {"quit", "exit", "q"}:
                print("æ‹œæ‹œ!")
                break
            # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºç©º
            if not user_input:
                print("è¯·è¾“å…¥èŠå¤©å†…å®¹ï¼")
                continue
            # å¤„ç†ç”¨æˆ·è¾“å…¥å¹¶é€‰æ‹©æ˜¯å¦æµå¼è¾“å‡ºå“åº”
            await graph_response(graph, user_input, config)

    except ConnectionPoolError as e:
        # æ•è·è¿æ¥æ± ç›¸å…³çš„å¼‚å¸¸
        logger.error(f"Connection pool error: {e}")
        print(f"é”™è¯¯: æ•°æ®åº“è¿æ¥æ± é—®é¢˜ - {e}")
        sys.exit(1)
    except RuntimeError as e:
        # æ•è·å…¶ä»–è¿è¡Œæ—¶é”™è¯¯
        logger.error(f"Initialization error: {e}")
        print(f"é”™è¯¯: åˆå§‹åŒ–å¤±è´¥ - {e}")
        sys.exit(1)
    except KeyboardInterrupt:
        # æ•è·é”®ç›˜ä¸­æ–­
        print("\nè¢«ç”¨æˆ·æ‰“æ–­ã€‚å†è§ï¼")
    except Exception as e:
        # æ•è·æœªé¢„æœŸçš„å…¶ä»–å¼‚å¸¸
        logger.error(f"Unexpected error: {e}")
        print(f"é”™è¯¯: å‘ç”ŸæœªçŸ¥é”™è¯¯ - {e}")
        sys.exit(1)
    finally:
        # æ¸…ç†èµ„æº
        if db_connection_pool and not db_connection_pool.closed:
            db_connection_pool.close()
            logger.info("Database connection pool closed")


# æ£€æŸ¥æ˜¯å¦ä¸ºä¸»æ¨¡å—è¿è¡Œ
if __name__ == "__main__":
    # è°ƒç”¨ä¸»å‡½æ•°
    asyncio.run(main())