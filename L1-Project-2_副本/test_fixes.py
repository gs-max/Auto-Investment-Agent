#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯async_agent_MCP.pyä¸­ä¸‰ä¸ªä¸»è¦é”™è¯¯çš„ä¿®å¤æ•ˆæœ
"""

import asyncio
import json
from langchain_core.messages import ToolMessage, AIMessage
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from async_agent_MCP import ParallelToolNode, grade_documents, rewrite, generate
from typing import Dict, Any, List

async def test_structured_tool_fix():
    """æµ‹è¯•StructuredToolåŒæ­¥è°ƒç”¨é—®é¢˜çš„ä¿®å¤"""
    print("=== æµ‹è¯•StructuredToolåŒæ­¥è°ƒç”¨ä¿®å¤ ===")
    
    # æ¨¡æ‹Ÿä¸€ä¸ªStructuredTool
    class MockStructuredTool:
        def __init__(self, name):
            self.name = name
            
        def run(self, args):
            return {"result": "success", "data": "test data"}
            
        def invoke(self, args):
            # æ¨¡æ‹ŸStructuredToolçš„åŒæ­¥è°ƒç”¨
            raise NotImplementedError('StructuredTool does not support sync invocation.')
    
    # åˆ›å»ºParallelToolNodeå®ä¾‹
    tool_node = ParallelToolNode()
    
    # æ¨¡æ‹Ÿå·¥å…·æ˜ å°„
    mock_tool = MockStructuredTool("test_tool")
    tool_map = {"test_tool": mock_tool}
    
    # æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨
    tool_call = {"name": "test_tool", "args": {"query": "test"}}
    
    try:
        result = await tool_node._run_single_tool_async(tool_call, tool_map)
        print(f"âœ… StructuredToolå¼‚æ­¥è°ƒç”¨æˆåŠŸ: {type(result)}")
        print(f"âœ… è¿”å›ToolMessageå¯¹è±¡: {isinstance(result, ToolMessage)}")
        return True
    except Exception as e:
        print(f"âŒ StructuredToolæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_none_score_fix():
    """æµ‹è¯•NoneTypeè¯„åˆ†é”™è¯¯çš„ä¿®å¤"""
    print("\n=== æµ‹è¯•NoneTypeè¯„åˆ†ä¿®å¤ ===")
    
    # æ¨¡æ‹ŸçŠ¶æ€
    mock_state = {
        "messages": [
            AIMessage(content="test question"),
            AIMessage(content="test context")
        ]
    }
    
    # æ¨¡æ‹ŸLLM
    class MockLLM:
        def __call__(self, *args, **kwargs):
            # æ¨¡æ‹Ÿè¿”å›Noneçš„æƒ…å†µ
            return None
    
    try:
        result = grade_documents(mock_state, MockLLM())
        print(f"âœ… NoneTypeè¯„åˆ†å¤„ç†æˆåŠŸ")
        print(f"âœ… è¿”å›æœ‰æ•ˆè¯„åˆ†: {result.get('relevance_score', 'no')}")
        return True
    except Exception as e:
        print(f"âŒ NoneTypeè¯„åˆ†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_json_serialization_fix():
    """æµ‹è¯•JSONåºåˆ—åŒ–é”™è¯¯çš„ä¿®å¤"""
    print("\n=== æµ‹è¯•JSONåºåˆ—åŒ–ä¿®å¤ ===")
    
    # æµ‹è¯•ToolMessageåºåˆ—åŒ–
    tool_msg = ToolMessage(content="test content", tool_call_id="test_id")
    
    try:
        # æµ‹è¯•æ˜¯å¦å¯ä»¥JSONåºåˆ—åŒ–
        json_str = json.dumps(tool_msg.content)
        print(f"âœ… ToolMessageå†…å®¹å¯åºåˆ—åŒ–: {json_str}")
        
        # æµ‹è¯•AIMessageåºåˆ—åŒ–
        ai_msg = AIMessage(content="test response")
        ai_json = json.dumps(ai_msg.content)
        print(f"âœ… AIMessageå†…å®¹å¯åºåˆ—åŒ–: {ai_json}")
        
        return True
    except Exception as e:
        print(f"âŒ JSONåºåˆ—åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹æµ‹è¯•async_agent_MCP.pyçš„ä¿®å¤æ•ˆæœ...\n")
    
    results = []
    
    # æµ‹è¯•1: StructuredToolåŒæ­¥è°ƒç”¨ä¿®å¤
    results.append(await test_structured_tool_fix())
    
    # æµ‹è¯•2: NoneTypeè¯„åˆ†ä¿®å¤
    results.append(test_none_score_fix())
    
    # æµ‹è¯•3: JSONåºåˆ—åŒ–ä¿®å¤
    results.append(test_json_serialization_fix())
    
    print(f"\n=== æµ‹è¯•ç»“æœæ±‡æ€» ===")
    print(f"æ€»æµ‹è¯•æ•°: {len(results)}")
    print(f"é€šè¿‡æµ‹è¯•: {sum(results)}")
    print(f"å¤±è´¥æµ‹è¯•: {len(results) - sum(results)}")
    
    if all(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¿®å¤æˆåŠŸ")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")

if __name__ == "__main__":
    asyncio.run(run_all_tests())
